{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Tutorial K3s","text":"<p>Este tutorial compila o tema \"Estudo de aplica\u00e7\u00e3o de cluster com computadores single board\"  que foi desenvolvido durante o Curso de Engenharia de Computa\u00e7\u00e3o na faculdade Impacta  pelos alunos.</p> <ul> <li>Filipe Verrone</li> <li>Leonardo Lins</li> <li>Ronaldo Mendes</li> <li>Yuri Mello</li> </ul> <p>Caso tenha interesse de enriquecer esta documenta\u00e7\u00e3o, fique a vontade para nos enviar um pull request.</p>"},{"location":"references/","title":"Refer\u00eancias","text":"<ul> <li>LinuxTips - Mais de 300 containers no cluster kubernetes com o K3s + Raspberry Pi 4</li> <li>NetworkChuck - I built a Raspberry Pi SUPER COMPUTER!! // ft. Kubernetes (K3s cluster w/ Rancher)</li> <li>Building a Raspberry Pi Cluster Part I - The Basics</li> <li>Building a Raspberry Pi Cluster Part II - Some Simple Jobs</li> <li>Building a Raspberry Pi Cluster Part III - OpenMPI, Python, and Parallel Jobs</li> </ul>"},{"location":"basics/orangepi/","title":"Configura\u00e7\u00f5es b\u00e1sicas - Orange Pi","text":"<p>O conte\u00fado a seguir tem como objetivo demonstrar como realizar as configura\u00e7\u00f5es b\u00e1sicas de rede no Orange Pi.</p>"},{"location":"basics/orangepi/#passo-01-instalar-o-sistema-operacional","title":"Passo 01: Instalar o sistema operacional","text":"<ul> <li> <p>Para encontrar o sistema operacional desejado, basta acessar a se\u00e7\u00e3o Downloads no site oficial do Orange Pi</p> </li> <li> <p>Ap\u00f3s realizar o download, utilize a ferramenta de sua prefer\u00eancia para gravar no SO em seu SD Card. ex: Balena Etcher, Rufus, etc</p> </li> <li> <p>Ao t\u00e9rmino da instala\u00e7\u00e3o, realize as atualiza\u00e7\u00f5es necess\u00e1rias no SO</p> </li> </ul>"},{"location":"basics/orangepi/#passo-02-endereco-ip-fixo","title":"Passo 02: Endere\u00e7o IP fixo","text":"<ul> <li>Realize as atualiza\u00e7\u00f5es de rede no arquivo <code>/etc/network/interfaces</code>, conforme o exemplo abaixo e salve as altera\u00e7\u00f5es em seguida <pre><code>#Static Ip Address\nauto eth0\niface eth0 inet static\naddress 192.168.0.30\nnetmask 255.255.255.0\ngateway 192.168.0.1\n</code></pre></li> </ul>"},{"location":"basics/orangepi/#passo-03-atualizar-o-hostname","title":"Passo 03: Atualizar o hostname","text":"<ul> <li>Utilize o comando <code>hostname nome_desejado</code> </li> <li>Acesse o arquivo em <code>/etc/hostname</code>, substitua pelo mesmo nome informado na etapa acima</li> <li>Ao t\u00e9rmino das atualiza\u00e7\u00f5es, use o comando <code>sudo reboot</code> para reiniciar o computador</li> </ul>"},{"location":"basics/raspberry/raspberrypi_os/","title":"Configura\u00e7\u00f5es b\u00e1sicas - Raspberry Pi","text":"<p>O conte\u00fado a seguir tem como objetivo demonstrar como realizar as configura\u00e7\u00f5es b\u00e1sicas de rede no Raspberry Pi.</p>"},{"location":"basics/raspberry/raspberrypi_os/#passo-01-instalar-o-sistema-operacional","title":"Passo 01: Instalar o sistema operacional","text":"<ul> <li> <p>Para encontrar o sistema operacional desejado, realizar o download do Raspberry Pi Imager na se\u00e7\u00e3o Software no site oficial do Raspberry Pi</p> </li> <li> <p>Siga os passos no Raspberry Pi Imager e realize a instala\u00e7\u00e3o do SO desejado</p> </li> <li> <p>Ao t\u00e9rmino da instala\u00e7\u00e3o, realize as atualiza\u00e7\u00f5es necess\u00e1rias no SO</p> </li> </ul>"},{"location":"basics/raspberry/raspberrypi_os/#passo-02-endereco-ip-fixo","title":"Passo 02: Endere\u00e7o IP fixo","text":""},{"location":"basics/raspberry/raspberrypi_os/#01-distribuicao-desktop","title":"01: Distribui\u00e7\u00e3o Desktop","text":"<ul> <li>Realize as atualiza\u00e7\u00f5es de rede no arquivo <code>/etc/dhcpcd.conf</code>, conforme o exemplo abaixo e salve as altera\u00e7\u00f5es em seguida <pre><code># fallback to static profile on eth0\ninterface eth0\nstatic ip_address=192.168.0.10\nstatic routers=192.168.0.1\nstatic domain_name_servers=192.168.0.1\nfallback static_eth0\n</code></pre></li> </ul>"},{"location":"basics/raspberry/raspberrypi_os/#02-distribuicao-server","title":"02: Distribui\u00e7\u00e3o Server","text":"<ul> <li>Edite as informa\u00e7\u00f5es ao final da linha no arquivo <code>/boot/cmdline.txt</code> com o conte\u00fado abaixo. Ao t\u00e9rmino da edi\u00e7\u00e3o, as informa\u00e7\u00f5es editadas devem ficar no formato do exemplo a seguir. <pre><code>#cgroup_memory=1 cgroup_enable=memory ip=ip_address::default_gateway:subnet_mask:seu_hostname:eth0:off\ncgroup_memory=1 cgroup_enable=memory ip=192.168.0.10::192.168.0.1:255.255.255.0:node-pi:eth0:off\n</code></pre></li> <li>Edite o arquivo <code>/boot/config.txt</code> e adicione a seguinte configura\u00e7\u00e3o ao final do arquivo <pre><code>arm_64bit=1\n</code></pre></li> <li>Crie um arquivo em branco ssh na pasta boot</li> <li>Como administrador, aplique o comando <code>sudo iptables -F</code></li> <li>Ao t\u00e9rmino das atualiza\u00e7\u00f5es, use o comando <code>sudo reboot</code> para reiniciar o computador</li> </ul>"},{"location":"basics/raspberry/raspberrypi_os/#passo-03-atualizar-o-hostname","title":"Passo 03: Atualizar o hostname","text":"<ul> <li>No Raspberry \u00e9 poss\u00edvel alterar atrav\u00e9s do Raspberry Pi Imager durante o processo de cria\u00e7\u00e3o da imagem</li> </ul>"},{"location":"basics/raspberry/ubuntu_server/","title":"Ubuntu Server [Extra]","text":"<p>Em breve...</p>"},{"location":"hpc/configs/","title":"Configura\u00e7\u00e3o b\u00e1sica - Single Board HPC Cluster","text":""},{"location":"hpc/configs/#passo-00-preparacao-do-hardware","title":"Passo 00: Prepara\u00e7\u00e3o do hardware","text":"<p>Como alternativa, iremos alterar o Raspberry Pi pelo Orange Pi, a fim de buscar por imcompatibilidades durante a constru\u00e7\u00e3o do cluster.</p> <ul> <li>3x Orange Pi Pc </li> <li>1x Raspberry Pi 3 Model B</li> <li>4x MicroSD Cards</li> <li>4x Fontes/Cabos de for\u00e7a</li> <li>1x Switch 8-portas 10/100/1000</li> <li>1x Pen Drive 64GB (estamos usando um menor com 16GB)</li> <li>Cabos de rede Cat5e</li> </ul>"},{"location":"hpc/configs/#passo-01-instalacao-do-sistema-operacional","title":"Passo 01: Instala\u00e7\u00e3o do Sistema Operacional","text":"<p>Durante essa etapa, basta seguir as instru\u00e7\u00f5es de outro tutorial presente neste reposit\u00f3rio para a instala\u00e7\u00e3o do sistema operacional e defini\u00e7\u00e3o dos hostnames de cada computador.</p> <p>Obs: Durante a defini\u00e7\u00e3o de hostnames, de prefer\u00eancia para nomes sem espa\u00e7os, ex: node01, node02, node03, node04.</p>"},{"location":"hpc/configs/#passo-02-sincronizacao-de-datahora","title":"Passo 02: Sincroniza\u00e7\u00e3o de data/hora","text":"<p>O SLURM e o MUNGE necessitam que as configura\u00e7\u00f5es de data/hora estejam devidamente sincronizadas, para esta etapa basta realizar a seguinte instala\u00e7\u00e3o: </p> <pre><code>sudo apt install ntpdate -y\n</code></pre> <p>Ao t\u00e9rmino do processo, basta reiniciar cada computador.</p>"},{"location":"hpc/configs/#passo-03-preparacao-do-armazenamento-compartilhado","title":"Passo 03: Prepara\u00e7\u00e3o do armazenamento compartilhado","text":"<ul> <li> <p>Conecte o pen drive no node01 que ser\u00e1 definido como Node Master</p> </li> <li> <p>Acesse o node01 atrav\u00e9s de outro terminal de forma remota</p> </li> <li> <p>Identifique as informa\u00e7\u00f5es do dispositivo m\u00f3vel com o comando a seguir: <pre><code>lsblk\n</code></pre></p> </li> <li> <p>Provavelmente o caminho do dispositivo ser\u00e1 /dev/sda1</p> </li> <li> <p>Realize a formata\u00e7\u00e3o do pen drive com o comando a seguir: <pre><code>sudo mkfs.ext4 /dev/sda1\n</code></pre></p> </li> <li> <p>Crie um novo diret\u00f3rio que servir\u00e1 como ponto de montagem entre todos os n\u00f3s, para isso, siga as instru\u00e7\u00f5es abaixo: <pre><code>sudo mkdir /clusterfs\nsudo chown nobody.nogroup -R /clusterfs\nsudo chmod 777 -R /clusterfs\n</code></pre></p> </li> <li> <p>Encontre as informa\u00e7\u00f5es de UUID do ponto /dev/sda1, para isso, utilize o comando a seguir: <pre><code>blkid\n</code></pre></p> </li> <li> <p>Resultado ser\u00e1 parecido com o demonstrado a seguir: <pre><code>/dev/sda1: UUID=\"017c30aa-4589-4ccb-a8b5-44d2834f8af8\"\n</code></pre></p> </li> <li> <p>A seguir edite o arquivo /etc/fstab, adicionando o UUID e outras informa\u00e7\u00f5es de acordo com o exemplo abaixo <pre><code>UUID=017c30aa-4589-4ccb-a8b5-44d2834f8af8 /clusterfs ext4 defaults 0 2\n</code></pre></p> </li> <li> <p>Por fim, monte a unidade com o comando a seguir: <pre><code>sudo mount -a\n</code></pre></p> </li> <li> <p>Antes de finalizar esta etapa, \u00e9 importante definir algumas permiss\u00f5es de acordo com os comandos a seguir: <pre><code>sudo chown nobody.nogroup -R /clusterfs\nsudo chmod -R 766 /clusterfs\n</code></pre></p> </li> </ul>"},{"location":"hpc/configs/#passo-04-configuracao-nfs-network-file-system","title":"Passo 04: Configura\u00e7\u00e3o NFS (Network File System)","text":""},{"location":"hpc/configs/#passo-01-instalacao-do-nfs-server","title":"Passo 01: Instala\u00e7\u00e3o do NFS Server","text":"<ul> <li> <p>Realize o processo de instala\u00e7\u00e3o no node01 com o comando a seguir: <pre><code>sudo apt install nfs-kernel-server -y\n</code></pre></p> </li> <li> <p>Edite o arquivo /etc/exports e adicione a linha abaixo: <pre><code>/clusterfs    &lt;ip addr&gt;(rw,sync,no_root_squash,no_subtree_check)\n</code></pre></p> </li> <li> <p>Substitua o  pelo endere\u00e7o IP inicial da sua rede, dessa forma, qualquer outro computador poder\u00e1 acessar o ponto de montagem. Por exemplo, se os endere\u00e7os da sua LAN forem no padr\u00e3o 192.168.1.X, o resultado ser\u00e1 conforme abaixo: <pre><code>/clusterfs 192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)\n</code></pre> <li> <p>Por fim, atualize o NFS kernel server com o comando a seguir: <pre><code>sudo exportfs -a\n</code></pre></p> </li>"},{"location":"hpc/configs/#passo-02-instalacao-do-nfs-client","title":"Passo 02: Instala\u00e7\u00e3o do NFS Client","text":"<ul> <li> <p>Realize o processo de instala\u00e7\u00e3o nos outros nodes com o comando a seguir: <pre><code>sudo apt install nfs-common -y\n</code></pre></p> </li> <li> <p>Crie o mesmo novo diret\u00f3rio que ser\u00e1 utilizado como ponto de montagem pelo node01, para isso, siga as instru\u00e7\u00f5es abaixo: <pre><code>sudo mkdir /clusterfs\nsudo chown nobody.nogroup -R /clusterfs\nsudo chmod 777 -R /clusterfs\n</code></pre></p> </li> <li> <p>Edite o arquivo /etc/fstab e adicione a seguinte linha: <pre><code>&lt;master node ip&gt;:/clusterfs    /clusterfs    nfs    defaults   0 0\n</code></pre></p> </li> <li> <p>A linha adicionada deve ficar conforme o exemplo abaixo: <pre><code>192.168.0.20:/clusterfs    /clusterfs    nfs    defaults   0 0\n</code></pre></p> </li> <li> <p>Por fim, para que seja poss\u00edvel criar arquivos e compartilhar entre todos os nodes, monte a unidade com o comando a seguir: <pre><code>sudo mount -a\n</code></pre></p> </li> </ul>"},{"location":"hpc/configs/#passo-05-configuracao-slurm-master-node","title":"Passo 05: Configura\u00e7\u00e3o SLURM (Master Node)","text":""},{"location":"hpc/configs/#passo-01-mapeamento-dos-hosts","title":"Passo 01: Mapeamento dos hosts","text":"<ul> <li>Para facilitar o mapeamento dos nodes \u00e9 necess\u00e1rio editar o arquivo /etc/hosts no node01 e adicionar as seguintes linhas: <pre><code>&lt;ip addr of node02&gt;      node02\n&lt;ip addr of node03&gt;      node03\n&lt;ip addr of node04&gt;      node04\n</code></pre></li> </ul>"},{"location":"hpc/configs/#passo-02-instalacao-do-slurm","title":"Passo 02: Instala\u00e7\u00e3o do SLURM","text":"<ul> <li> <p>No node01 realize a instala\u00e7\u00e3o com o seguinte comando: <pre><code>sudo apt install slurm-wlm -y\n</code></pre></p> </li> <li> <p>Fa\u00e7a uma c\u00f3pia, extraia os arquivos base com os comandos a seguir: <pre><code>cd /etc/slurm-llnl\ncp /usr/share/doc/slurm-client/examples/slurm.conf.simple.gz .\ngzip -d slurm.conf.simple.gz\nmv slurm.conf.simple slurm.conf\n</code></pre></p> </li> <li> <p>Edite o arquivo /etc/slurm-llnl/slurm.conf, adicionando as informa\u00e7\u00f5es abaixo: <pre><code>ControlMachine=node01\nControlAddr=192.168.0.20\n\nSelectType=select/cons_res\nSelectTypeParameters=CR_Core\nClusterName=HPCluster\n\nNodeName=node01 NodeAddr=192.168.0.20 CPUs=4 State=UNKNOWN\nNodeName=node02 NodeAddr=192.168.0.21 CPUs=4 State=UNKNOWN\nNodeName=node03 NodeAddr=192.168.0.22 CPUs=4 State=UNKNOWN\n\nPartitionName=mycluster Nodes=node[02-03] Default=YES MaxTime=INFINITE State=UP\n</code></pre> Obs: Dependendo da vers\u00e3o do SLURM as configura\u00e7\u00f5es de ControlMachine e ControlAddr dever\u00e3o ser substitu\u00eddas pela seguinte configura\u00e7\u00e3o: <pre><code>SlurmctldHost=node01(&lt;ip addr of node01&gt;)\n# e.g.: node01(192.168.1.14)\n</code></pre></p> </li> <li> <p>Crie o arquivo /etc/slurm-llnl/cgroup.conf com as seguintes configura\u00e7\u00f5es: <pre><code>CgroupMountpoint=\"/sys/fs/cgroup\"\nCgroupAutomount=yes\nCgroupReleaseAgentDir=\"/etc/slurm-llnl/cgroup\"\nAllowedDevicesFile=\"/etc/slurm-llnl/cgroup_allowed_devices_file.conf\"\nConstrainCores=no\nTaskAffinity=no\nConstrainRAMSpace=yes\nConstrainSwapSpace=no\nConstrainDevices=no\nAllowedRamSpace=100\nAllowedSwapSpace=0\nMaxRAMPercent=100\nMaxSwapPercent=100\nMinRAMSpace=30\n</code></pre></p> </li> <li> <p>Por fim, crie o arquivo /etc/slurm-llnl/cgroup_allowed_devices_file.conf e adicione as seguintes informa\u00e7\u00f5es: <pre><code>/dev/null\n/dev/urandom\n/dev/zero\n/dev/sda*\n/dev/cpu/*/*\n/dev/pts/*\n/clusterfs*\n</code></pre></p> </li> </ul>"},{"location":"hpc/configs/#passo-03-copiar-os-arquivos-de-configuracao-para-a-pasta-compartilhada","title":"Passo 03: Copiar os arquivos de configura\u00e7\u00e3o para a pasta compartilhada","text":"<ul> <li>Realize os comandos a seguir para copiar os arquivos de configura\u00e7\u00e3o e a chave de autentica\u00e7\u00e3o do Munge <pre><code>sudo cp slurm.conf cgroup.conf cgroup_allowed_devices_file.conf /clusterfs\nsudo cp /etc/munge/munge.key /clusterfs\n</code></pre></li> </ul>"},{"location":"hpc/configs/#passo-04-iniciar-e-habilitar-os-servicos-do-slurm","title":"Passo 04: Iniciar e habilitar os servi\u00e7os do SLURM","text":"<ul> <li> <p>Ative o Munge: <pre><code>sudo systemctl enable munge\nsudo systemctl start munge\n</code></pre></p> </li> <li> <p>Ative o SLURM daemon: <pre><code>sudo systemctl enable slurmd\nsudo systemctl start slurmd\n</code></pre></p> </li> <li> <p>Ative o control daemon: <pre><code>sudo systemctl enable slurmctld\nsudo systemctl start slurmctld\n</code></pre></p> </li> </ul>"},{"location":"hpc/configs/#passo-05-reiniciar-opcional","title":"Passo 05: Reiniciar (Opcional)","text":"<ul> <li>Caso ocorra algum problema com a autentica\u00e7\u00e3o do Munge ou n\u00e3o houver comunica\u00e7\u00e3o com o SLURM Controller, basta reiniciar o node01</li> </ul>"},{"location":"hpc/configs/#passo-06-configuracao-slurm-nodes","title":"Passo 06: Configura\u00e7\u00e3o SLURM (Nodes)","text":""},{"location":"hpc/configs/#passo-01-instalacao-do-slurm-client","title":"Passo 01: Instala\u00e7\u00e3o do SLURM Client","text":"<ul> <li> <p>Em cada um dos nodes realize a instala\u00e7\u00e3o com o seguinte comando: <pre><code>sudo apt install slurmd slurm-client -y\n</code></pre></p> </li> <li> <p>Para facilitar o mapeamento dos nodes da mesma forma que foi realizado no node01, para isso, edite o arquivo /etc/hosts e adicione as seguintes linhas: <pre><code>#node02:/etc/hosts\n&lt;ip addr of node01&gt;      node01\n&lt;ip addr of node03&gt;      node03\n&lt;ip addr of node04&gt;      node04\n</code></pre></p> </li> <li> <p>Copie os arquivos de configura\u00e7\u00e3o que foram compartilhados em /clusterfs, para isso siga os seguintes passos: <pre><code>sudo cp /clusterfs/munge.key /etc/munge/munge.key\nsudo cp /clusterfs/slurm.conf /etc/slurm-llnl/slurm.conf\nsudo cp /clusterfs/cgroup* /etc/slurm-llnl\n</code></pre></p> </li> </ul>"},{"location":"hpc/configs/#passo-02-testando-o-munge","title":"Passo 02: Testando o Munge","text":"<ul> <li> <p>Para testar a chave Munge que foi copiada, inicie e ative-o com os seguintes comandos: <pre><code>sudo systemctl enable munge\nsudo systemctl start munge\n</code></pre></p> </li> <li> <p>Acesse cada um dos nodes, realizando o comando abaixo para validar se a autentica\u00e7\u00e3o Munge, que foi configurada no node01 est\u00e1 funcionando corretamente <pre><code># conectando em um Raspberry\nssh pi@node01 munge -n | unmunge\n\n# conectando em um Orange\nssh root@node01 munge -n | unmunge\n</code></pre></p> </li> <li> <p>Caso a autentica\u00e7\u00e3o funcione, o resultado ser\u00e1 parecido com o exemplo abaixo: <pre><code>root@node01's password:\nSTATUS:           Success (0)\nENCODE_HOST:      node01 (192.168.0.20)\nENCODE_TIME:      2023-11-18 02:45:52 -0300 (1700286352)\nDECODE_TIME:      2023-11-18 02:45:52 -0300 (1700286352)\nTTL:              300\nCIPHER:           aes128 (4)\nMAC:              sha256 (5)\nZIP:              none (0)\nUID:              root (0)\nGID:              root (0)\nLENGTH:           0\n</code></pre> Obs: Caso o processo n\u00e3o funcione, reinicie todos os nodes e tente novamente</p> </li> </ul>"},{"location":"hpc/configs/#passo-03-ativacao-do-slurm","title":"Passo 03: Ativa\u00e7\u00e3o do SLURM","text":"<ul> <li>Ative o SLURM daemon: <pre><code>sudo systemctl enable slurmd\nsudo systemctl start slurmd\n</code></pre></li> </ul>"},{"location":"hpc/configs/#passo-07-testando-o-slurm","title":"Passo 07: Testando o SLURM","text":"<ul> <li>No node01, digite o comando <code>sinfo</code> para verificar o status atual de cada node. O status desejado deve estar da seguinte forma: <pre><code>PARTITION  AVAIL  TIMELIMIT  NODES  STATE NODELIST\nmycluster*    up   infinite      3   idle node[02-04]\n</code></pre></li> <li> <p>Caso ocorra algum erro, tente atualizar o status de cada node com o comando abaixo: <pre><code># substitua \"node00\" pelo nome do node desejado\nscontrol update nodename=node00 state=idle\n</code></pre></p> </li> <li> <p>A seguir, realize um teste para que retorne os nomes dos nodes com o comando <code>hostname</code>. Para isso, digite o comando abaixo no node01 <pre><code>srun --nodes=3 hostname\n</code></pre></p> </li> <li>O resultado ser\u00e1 mostrado da seguinte forma: <pre><code>node02\nnode03\nnode04\n</code></pre></li> </ul>"},{"location":"hpc/configs/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Visualizar se a pasta compartilhada existe:  <pre><code>df -h\n</code></pre></p> </li> <li> <p>For\u00e7ar a montagem da pasta compartilhada:  <pre><code>mount -t nfs &lt;nfs_server_ip&gt;:&lt;shared_folder_path&gt; &lt;mount_point&gt;\n</code></pre></p> </li> <li> <p>For\u00e7ar a altera\u00e7\u00e3o de status dos nodes: <pre><code># substitua \"node00\" pelo nome do node desejado\nscontrol update nodename=node00 state=idle\n</code></pre></p> </li> </ul>"},{"location":"hpc/open_mpi/","title":"OpenMPI","text":""},{"location":"hpc/open_mpi/#passo-01-instalacao-do-openmpi","title":"Passo 01: Instala\u00e7\u00e3o do OpenMPI","text":"<ul> <li> <p>Acesse o node01 e realize a instala\u00e7\u00e3o do OpenMPI e suas depend\u00eancias em cada node. Para isso, utilize os seguintes comandos: <pre><code>sudo su -\nsrun --nodes=3 apt install openmpi-bin openmpi-common libopenmpi3 libopenmpi-dev -y\n</code></pre></p> </li> <li> <p>A seguir, realize o primeiro teste criando o arquivo /clusterfs/hello_mpi.c com o seguinte conte\u00fado: <pre><code>#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\nint main(int argc, char** argv) {\n    int node;\n    MPI_Init(&amp;argc, &amp;argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;node);\n    printf(\"Hello World from Node %d!\\n\", node);\n    MPI_Finalize();\n}\n</code></pre></p> </li> <li> <p>Em seguida \u00e9 necess\u00e1rio compilar o c\u00f3digo em C para que seja poss\u00edvel rodar em cada node a partir do arquivo a.out. Para isso, realize as seguintes etapas: <pre><code>login1$ srun --pty bash\nnode1$ cd /clusterfs\nnode1$ mpicc hello_mpi.c\nnode1$ ls\na.out* hello_mpi.c\nnode1$ exit\n</code></pre></p> </li> <li> <p>Crie um script com o nome /clusterfs/sub_mpi.sh para que seja poss\u00edvel rodar no cluster. Utilize o conte\u00fado a seguir: <pre><code>#!/bin/bash\ncd $SLURM_SUBMIT_DIR\n# Print the node that starts the process\necho \"Master node: $(hostname)\"\n# Run our program using OpenMPI.\n# OpenMPI will automatically discover resources from SLURM.\nmpirun --allow-run-as-root a.out\n</code></pre></p> </li> <li> <p>Por fim, execute o job seguindo os seguintes comandos: <pre><code>cd /clusterfs\nsbatch --nodes=3 --ntasks-per-node=2 sub_mpi.sh\n</code></pre></p> </li> <li> <p>O resultado final deve ser parecido com o informado abaixo: <pre><code>Master node: node01\nHello World from Node 0!\nHello World from Node 1!\nHello World from Node 2!\nHello World from Node 3!\nHello World from Node 4!\nHello World from Node 5!\n</code></pre></p> </li> </ul>"},{"location":"hpc/python_lang/","title":"Instala\u00e7\u00e3o Python","text":""},{"location":"hpc/python_lang/#passo-01-instalando-dependencias","title":"Passo 01: Instalando depend\u00eancias","text":"<ul> <li>Instale as bibliotecas do Python no node01, seguindo o comando abaixo: <pre><code>sudo apt install -y build-essential python-dev python-setuptools python-pip python-smbus libncursesw5-dev libgdbm-dev libc6-dev zlib1g-dev libsqlite3-dev tk-dev libssl-dev openssl libffi-dev\n</code></pre></li> </ul>"},{"location":"hpc/python_lang/#passo-02-configuracao-e-build-do-python","title":"Passo 02: Configura\u00e7\u00e3o e build do Python","text":"<ul> <li> <p>Realize o download e descompacte o Python na vers\u00e3o indicada abaixo: <pre><code>cd /clusterfs &amp;&amp; mkdir build &amp;&amp; cd build\nwget https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tgz\ntar xvzf Python-3.7.3.tgz\n... tar output ...\ncd Python-3.7.3\n</code></pre></p> </li> <li> <p>Configure o Python no node01 conforme os passos abaixo: <pre><code>mkdir /clusterfs/usr        # directory Python will install to\n$ cd /clusterfs/build/Python-3.7.3\n$ srun --nodelist=node01 bash  # configure will be run on node01\nnode01$ ./configure --enable-optimizations --prefix=/clusterfs/usr --with-ensurepip=install\n</code></pre></p> </li> <li> <p>Crie um novo script com o nome sub_build_python.sh com as instru\u00e7\u00f5es abaixo: <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodelist=node1\ncd $SLURM_SUBMIT_DIR\nmake -j4\n</code></pre></p> </li> <li> <p>A seguir execute acesse a pasta com a vers\u00e3o do Python e execute o script utilizando os recursos do SLURM:  <pre><code>cd /clusterfs/build/Python-3.7.3\nsbatch sub_build_python.sh\n</code></pre></p> </li> <li> <p>Para visualizar os logs da instala\u00e7\u00e3o, utilize o comando abaixo: <pre><code>tail -f slurm-xxx.out # xxx \u00e9 o n\u00famero do JOBID\n</code></pre></p> </li> </ul>"},{"location":"hpc/python_lang/#passo-03-instalacao-do-python","title":"Passo 03: Instala\u00e7\u00e3o do Python","text":"<ul> <li> <p>Crie um novo arquivo com o nome sub_install_python.sh no diret\u00f3rio /clusterfs/build/Python-3.7.3 com as seguintes instru\u00e7\u00f5es: <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --nodelist=node01\ncd $SLURM_SUBMIT_DIR\nmake install\n</code></pre></p> </li> <li> <p>Execute o novo script seguindo os passos abaixo: <pre><code>sudo su -\ncd /clusterfs/build/Python-3.7.3\nsbatch sub_install_python.sh\n</code></pre></p> </li> <li> <p>Tamb\u00e9m \u00e9 poss\u00edvel visualizar os logs da instala\u00e7\u00e3o utilizando o comando abaixo: <pre><code>tail -f slurm-xxx.out # xxx \u00e9 o n\u00famero do JOBID\n</code></pre></p> </li> </ul>"},{"location":"hpc/python_lang/#passo-04-testes","title":"Passo 04: Testes","text":"<ul> <li> <p>Realize um teste para verificar se o Python foi devidamente instalado no dispositivo compartilhado com o seguinte comando: <pre><code>srun --nodes=3 /clusterfs/usr/bin/python3 -c \"print('Hello')\"\n# Hello\n# Hello\n# Hello\n</code></pre></p> </li> <li> <p>Por fim, verifique se o pip est\u00e1 acess\u00edvel com o comando abaixo: <pre><code>srun --nodes=1 /clusterfs/usr/bin/pip3 --version\n# pip 19.0.3 from /clusterfs/usr/lib/python3.7/site-packages/pip (python 3.7)\n</code></pre></p> </li> </ul>"},{"location":"hpc/r_lang/","title":"Gerando gr\u00e1ficos de distribui\u00e7\u00e3o e histogramas com a linguagem R","text":"<p>Nesta etapa iremos descrever como gerar gr\u00e1ficos de distribui\u00e7\u00e3o e histogramas utilizando a linguagem R.</p>"},{"location":"hpc/r_lang/#passo-01-instalando-o-r","title":"Passo 01: Instalando o R","text":"<ul> <li> <p>Acesse o node01 e fa\u00e7a a instala\u00e7\u00e3o de forma remota nos outros nodes (indicando quantidade que voc\u00ea tiver dispon\u00edvel) atrav\u00e9s do seguinte comando: <pre><code>sudo su -\nsrun --nodes=3 apt install r-base -y\n</code></pre></p> </li> <li> <p>Ao t\u00e9rmino do processo, acesse os outros nodes e digite o comando <code>R --version</code> para visualizar o resultado que ser\u00e1 parecido com o exemplo abaixo: <pre><code>root@node02:~# R --version\nR version 3.4.4 (2018-03-15) -- \"Someone to Lean On\"\nCopyright (C) 2018 The R Foundation for Statistical Computing\nPlatform: arm-unknown-linux-gnueabihf (32-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under the terms of the\nGNU General Public License versions 2 or 3.\nFor more information about these matters see\nhttp://www.gnu.org/licenses/.\n</code></pre></p> </li> </ul>"},{"location":"hpc/r_lang/#passo-03-criando-o-algoritmo-em-r-para-teste","title":"Passo 03: Criando o algoritmo em R para teste","text":"<ul> <li> <p>Crie um arquivo em /clusterfs/normal/generate.R com as seguintes informa\u00e7\u00f5es: <pre><code>arg = commandArgs(TRUE)\nsamples = rep(NA, 100000)\nfor (i in 1:100000){ samples[i] = mean(rexp(40, 0.2)) }\njpeg(paste('plots/', arg, '.jpg', sep=\"\"))\nhist(samples, main=\"\", prob=T, color=\"darkred\")\nlines(density(samples), col=\"darkblue\", lwd=3)\ndev.off()\n</code></pre></p> </li> <li> <p>A seguir, crie uma nova pasta chamada plots e execute o comando a abaixo em um dos nodes: <pre><code>mkdir plots\nR --vanilla -f generate.R --args \"plot1\"\n</code></pre></p> </li> <li> <p>Ao acessar a pasta plots ser\u00e1 poss\u00edvel ver o resultado no arquivo plot1.jpg</p> </li> </ul>"},{"location":"hpc/r_lang/#passo-03-criando-um-script-batch-com-codigo-em-r","title":"Passo 03: Criando um script batch com c\u00f3digo em R","text":"<ul> <li> <p>Crie um novo arquivo /clusterfs/normal/submit.sh com as seguintes informa\u00e7\u00f5es: <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --partition=&lt;partition name&gt;\ncd $SLURM_SUBMIT_DIR\nmkdir plots\nR --vanilla -f generate.R --args \"plot$SLURM_ARRAY_TASK_ID\"\n</code></pre></p> </li> <li> <p>Acesse o node01 e rode o job criado no passo anterior. Para isso, siga os passos abaixo: <pre><code>cd /clusterfs/normal\nsbatch --array=[1-50] submit.sh\n</code></pre></p> </li> <li> <p>Ap\u00f3s a inicializa\u00e7\u00e3o do job, execute o comando <code>squeue</code>. O retorno ser\u00e1 conforme abaixo: <pre><code>    JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n11_[9-50] mycluster submit.s     root PD       0:00      1 (Resources)\n     11_1 mycluster submit.s     root  R       0:03      1 node02\n     11_2 mycluster submit.s     root  R       0:03      1 node02\n     11_3 mycluster submit.s     root  R       0:03      1 node02\n     11_4 mycluster submit.s     root  R       0:03      1 node02\n     11_5 mycluster submit.s     root  R       0:03      1 node03\n     11_6 mycluster submit.s     root  R       0:03      1 node03\n     11_7 mycluster submit.s     root  R       0:03      1 node03\n     11_8 mycluster submit.s     root  R       0:03      1 node03\n</code></pre></p> </li> <li> <p>Por fim, acesso o diret\u00f3rio /clusterfs/normal/plots para visualizar os plots que foram criados</p> </li> </ul>"},{"location":"hpc/slurm/","title":"Comandos b\u00e1sicos - SLURM","text":"<ul> <li> <p>sinfo: disponibiliza informa\u00e7\u00f5es relacionadas ao cluster <pre><code>$ sinfo\nPARTITION   AVAIL   TIMELIMIT   NODES   STATE   NODELIST\nmycluster*     up    infinite       3    idle   node[01\u201303]\n</code></pre></p> </li> <li> <p>scontrol: com este comando \u00e9 poss\u00edvel visualizar o status atual do job que est\u00e1 sendo executado, al\u00e9m de atualizar o status dos nodes caso estejam em algum status diferente de idle <pre><code># atualiza o status dos n\u00f3s que comp\u00f5em o cluster\nscontrol update nodename=node[02-03] state=idle\n\n# disponibiliza o status atual do job\nscontrol show job &lt;JOBID&gt;\n</code></pre></p> </li> <li> <p>srun: \u00e9 utilizado para executar um comando diretamente em quantos nodes/n\u00facleos voc\u00ea desejar <pre><code>$ srun --nodes=3 hostname\nnode01\nnode02\nnode03\n</code></pre></p> </li> <li> <p>Tamb\u00e9m \u00e9 poss\u00edvel executar as tarefas em um \u00fanico node, dessa forma o resultado ser\u00e1 diferente do exemplo acima <pre><code>$ srun --ntasks=3 hostname\nnode02\nnode02\nnode02\n</code></pre></p> </li> <li> <p>Outro par\u00e2metro importante \u00e9 o ntasks, com ele \u00e9 poss\u00edvel indicar a quantidade de tarefas por node <pre><code>$ srun --nodes=2 --ntasks-per-node=3 hostname\nnode02\nnode02\nnode02\nnode03\nnode03\nnode03\n</code></pre></p> </li> <li> <p>squeue: \u00e9 utilizado para visualizar os jobs agendados <pre><code>$ squeue\nJOBID PARTITION  NAME     USER ST    TIME  NODES NODELIST(REASON)\n  609 mycluster  24.sub.s pi   R     10:16     1 node02\n</code></pre></p> </li> <li> <p>scancel: \u00e9 utilizado para cancelar um job em execu\u00e7\u00e3o <pre><code># scancel JOBID\nscancel 609\n</code></pre></p> </li> <li> <p>sbatch: \u00e9 utilizado para executar um script batch</p> </li> <li> <p>Para exemplificar, crie um arquivo /clusterfs/helloworld.sh e adicione as seguintes informa\u00e7\u00f5es (n\u00e3o esque\u00e7a de substituir o partition name pelo nome do da sua PARTITION, ex: mycluster): <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --partition=&lt;partition name&gt;\ncd $SLURM_SUBMIT_DIR\necho \"Hello, World!\" &gt; helloworld.txt\n</code></pre></p> </li> <li> <p>Por fim, basta digitar o comando a seguir para que o job seja executado: <pre><code>sbatch ./helloworld.sh\n</code></pre></p> </li> <li> <p>N\u00e3o ir\u00e1 aparecer nenhuma mensagem na tela, no entanto, ser\u00e1 poss\u00edvel visualizar que um novo arquivo foi criado /clusterfs/helloworld.txt</p> </li> <li> <p>Em caso de erros, um arquivo com o nome slurm-XXX.out, onde XXX \u00e9 o c\u00f3digo do job que foi executado</p> </li> </ul>"},{"location":"hpc/tests/","title":"Testes Python e MPI","text":""},{"location":"hpc/tests/#passo-01-instalacao-de-bibliotecas-python","title":"Passo 01: Instala\u00e7\u00e3o de bibliotecas Python","text":"<ul> <li> <p>Crie um novo arquivo em /clusterfs/calc-pi/sub_install_pip.sh e adicione as seguintes informa\u00e7\u00f5es: <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n/clusterfs/usr/bin/pip3 install numpy mpi4py\n</code></pre></p> </li> <li> <p>Execute o job como adminstrador para que seja poss\u00edvel realizar as modifica\u00e7\u00f5es na vers\u00e3o Python instalada: <pre><code>cd /clusterfs/calc-pi\nsudo su\nsbatch sub_install_pip.sh\n</code></pre></p> </li> <li> <p>aaa <pre><code>from mpi4py import MPI\nfrom math   import pi as PI\nfrom numpy  import array\n\ndef comp_pi(n, myrank=0, nprocs=1):\n    h = 1.0 / n\n    s = 0.0\n    for i in range(myrank + 1, n + 1, nprocs):\n        x = h * (i - 0.5)\n        s += 4.0 / (1.0 + x**2)\n    return s * h\n\ndef prn_pi(pi, PI):\n    message = \"pi is approximately %.16f, error is %.16f\"\n    print  (message % (pi, abs(pi - PI)))\n\ncomm = MPI.COMM_WORLD\nnprocs = comm.Get_size()\nmyrank = comm.Get_rank()\n\nn    = array(0, dtype=int)\npi   = array(0, dtype=float)\nmypi = array(0, dtype=float)\n\nif myrank == 0:\n    _n = 20 # Enter the number of intervals\n    n.fill(_n)\ncomm.Bcast([n, MPI.INT], root=0)\n_mypi = comp_pi(n, myrank, nprocs)\nmypi.fill(_mypi)\ncomm.Reduce([mypi, MPI.DOUBLE], [pi, MPI.DOUBLE],\n            op=MPI.SUM, root=0)\nif myrank == 0:\n    prn_pi(pi, PI)\n</code></pre></p> </li> </ul>"},{"location":"lbc/basic_k3s/","title":"Primeiros passos - Cluster com K3s","text":"<p>Neste tutorial introdut\u00f3rio ser\u00e3o apresentados os passos iniciais de utiliza\u00e7\u00e3o do K3s para a cria\u00e7\u00e3o de um ambiente clusterizado em Single-Board Computers (SBC) como Raspberry Pi, Orange Pi, entre outros. Para maiores informa\u00e7\u00f5es sobre o K3s, basta acessar a documenta\u00e7\u00e3o oficial.</p> <p>Durante este tutorial ser\u00e3o abordados dois tipos de m\u00e1quinas: </p> <ul> <li> <p>Servers - s\u00e3o as m\u00e1quinas onde o K3s ser\u00e1 instalado e ter\u00e3o as configura\u00e7\u00f5es principais com os manifestos yaml, contendo todas as informa\u00e7\u00f5es para o deploy de aplica\u00e7\u00f5es, podendo ser sites com p\u00e1ginas est\u00e1ticas, at\u00e9 projetos maiores envolvendo banco de dados.</p> </li> <li> <p>Agents ou Workers - s\u00e3o m\u00e1quinas consideradas como n\u00f3s, onde ir\u00e3o adicionar de forma complementar novas inst\u00e2ncias dos servi\u00e7os configurados nos Servers. Ex.: em casos de servi\u00e7os como Load-Balancer, quando maior a quantidade de m\u00e1quinas, maior a disponibilidade de escalonamento e acesso a aplica\u00e7\u00e3o </p> </li> </ul>"},{"location":"lbc/basic_k3s/#passo-01-preparacao-de-redes","title":"Passo 01: Prepara\u00e7\u00e3o de redes","text":"<ul> <li>Neste passo, \u00e9 de suma import\u00e2ncia que os IPs de cada computador seja est\u00e1tico. Para isso, basta seguir os passos de configura\u00e7\u00f5es de rede neste reposit\u00f3rio</li> </ul>"},{"location":"lbc/basic_k3s/#passo-02-instalando-o-k3s-em-uma-maquina-server","title":"Passo 02: Instalando o K3s em uma m\u00e1quina Server","text":"<ul> <li> <p>Ap\u00f3s acessar o terminal, sendo de forma remota, por ssh ou com o SBC conectado em um monitor, digite o comando abaixo: <pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre></p> </li> <li> <p>Caso opte por remover algum recurso complementar que \u00e9 adicionado durante a instala\u00e7\u00e3o, como o Traefik, o comando pode ser realizado dessa forma: <pre><code>curl -sfL https://get.k3s.io | sh -s - --disable=traefik\n</code></pre></p> </li> <li> <p>Depois de realizar a instala\u00e7\u00e3o do K3s, aguarde aproximadamente 30 segundos para visualizar as informa\u00e7\u00f5es do server com o comando: <pre><code>kubectl get nodes\n</code></pre></p> </li> <li> <p>Para verificar todos os pods dispon\u00edveis, digite o comando a seguir: <pre><code>kubectl get pods --all-namespaces\n</code></pre></p> </li> <li> <p>Para listar todas os status gerais do cluster, basta digitar o comando abaixo: <pre><code>kubectl get all --all-namespaces\n</code></pre></p> </li> <li> <p>Ap\u00f3s realizar do status atual do server, obtenha o token que ser\u00e1 utilizado durante a instala\u00e7\u00e3o do K3s nas m\u00e1quinas workers. Para isso, digite o comando a seguir e copiei a resposta obtida:  <pre><code>cat /var/lib/rancher/k3s/server/node-token\n</code></pre></p> </li> <li> <p>Por fim, obtenha o ip do server com o comando <code>ifconfig</code> e anote as informa\u00e7\u00f5es destas \u00faltimas duas etapas para o pr\u00f3ximo passo.</p> </li> </ul>"},{"location":"lbc/basic_k3s/#passo-03-instalando-o-k3s-em-uma-maquina-worker","title":"Passo 03: Instalando o K3s em uma m\u00e1quina Worker","text":"<ul> <li> <p>Ap\u00f3s obter o token do K3s e o ip do server no passo anterior, edite o comando abaixo, onde se encontram as vari\u00e1veis server_ip e server_token para realizar a instala\u00e7\u00e3o do K3s na quantidade de workers que desejar <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://server_ip:6443 K3S_TOKEN=server_token sh -\n</code></pre></p> </li> <li> <p>No server, utilize o comando a seguir para visualizar a atualiza\u00e7\u00e3o dos node:  <pre><code>watch -n 1 kubectl get nodes\n</code></pre></p> </li> <li> <p>Importante: Para configurar um worker no Raspberry Pi, ser\u00e1 necess\u00e1rio incluir as informa\u00e7\u00f5es <code>cgroup_memory=1 cgroup_enable=memory</code> ao final do arquivo <code>/boot/cmdline.txt</code> caso ainda n\u00e3o existam.</p> </li> </ul>"},{"location":"lbc/basic_k3s/#passo-04-opcional-desinstalando-o-k3s","title":"Passo 04 (Opcional): Desinstalando o K3s","text":"<ul> <li> <p>Em uma m\u00e1quina server, utilize o comando:  <pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre></p> </li> <li> <p>Em cada m\u00e1quina worker, utilize o comando:  <pre><code>/usr/local/bin/k3s-agent-uninstall.sh\n</code></pre></p> </li> </ul>"},{"location":"lbc/basic_k3s/#passo-05-opcional-solucionando-problemas","title":"Passo 05 (Opcional): Solucionando problemas","text":"<ul> <li> <p>Caso seja necess\u00e1rio parar o server, use o comando:  <pre><code>/usr/local/bin/k3s-killall.sh\n</code></pre></p> </li> <li> <p>Para reiniciar o server:  <pre><code>sudo systemctl restart k3s\n</code></pre></p> </li> <li> <p>Para reiniciar o agent:  <pre><code>sudo systemctl restart k3s-agent\n</code></pre></p> </li> </ul>"},{"location":"lbc/basic_k3s/#passo-06-opcional-openlens-dashboard","title":"Passo 06 (Opcional): OpenLens Dashboard","text":"<p>O OpenLens \u00e9 uma ferramenta de grande import\u00e2ncia para garantir maior visibilidade, estat\u00edsticas em tempo real, estado atual dos servi\u00e7os que est\u00e3o funcionando no cluster, al\u00e9m de facilitar na solu\u00e7\u00e3o de problemas.</p> <ul> <li> <p>Em um computador conectado na mesma rede onde o cluster est\u00e1 sendo configurado, fa\u00e7a o download do OpenLens para o sistema operacional desejado e realize a instala\u00e7\u00e3o</p> </li> <li> <p>Ap\u00f3s realizar a instala\u00e7\u00e3o do OpenLens, selecione a op\u00e7\u00e3o Add Cluster</p> </li> <li> <p>Acesse o server atrav\u00e9s do prompt de comando e digite o comando abaixo:  <pre><code>kubectl config view --minify --raw\n</code></pre></p> </li> <li> <p>Copie os dados que foram exibidos no terminal e cole no OpenLens</p> </li> <li> <p>Altere o IP 127.0.0.1:6443 para o IP do server onde o K3s foi instalado</p> </li> </ul>"},{"location":"lbc/commands/","title":"Comandos b\u00e1sicos para gerenciar os deploys no K3s","text":"<p>O tutorial a seguir tem como objetivo auxiliar com instru\u00e7\u00f5es para gerenciar aplica\u00e7\u00f5es implantadas (deploys) no cluster com K3s</p> <ul> <li>Remover um deployment realizado: Em arquivos com m\u00faltiplas implementa\u00e7\u00f5es (Deployment, Service, Ingress, etc...) com o commando abaixo \u00e9 poss\u00edvel remover tudo o que foi implementado. <pre><code>kubectl delete -f file_name.yaml\n</code></pre></li> </ul>"},{"location":"lbc/manifests/","title":"Fazendo o deploy de aplica\u00e7\u00f5es e servi\u00e7os no K3s","text":"<p>Nesta etapa do tutorial, \u00e9 importante que voc\u00ea j\u00e1 tenha o K3s instalado e configurado. Em seguida, basta seguir as etapas para a implementa\u00e7\u00e3o dos servi\u00e7os desejados.</p> <p>Para utilizar o arquivo pandaweb.yaml, basta aplicar as configura\u00e7\u00f5es com um dos comandos abaixo: <pre><code>kubectl create -f pandaweb.yaml --save-config\n</code></pre></p> <p>ou</p> <p><pre><code>kubectl apply -f pandaweb.yaml\n</code></pre> A \u00fanica diferen\u00e7a entre os comandos \u00e9 que o segundo gera uma notifica\u00e7\u00e3o referente as configura\u00e7\u00f5es realizadas durante o deploy.</p> <p>Para utilizar o arquivo rancherdemo.yaml, basta seguir o mesmo passo acima. Caso deseje alterar alguma informa\u00e7\u00e3o na se\u00e7\u00e3o demonstrada abaixo, basta seguir as configura\u00e7\u00f5es que ser\u00e3o apresentadas ap\u00f3s o trecho de c\u00f3digo.</p> <pre><code>env:\n- name: CONTAINER_COLOR\nvalue: orange\n- name: PETS\nvalue: chameleons\n- name: TITLE\nvalue: TCC Load Balancer Demo\n</code></pre> <ul> <li> <p>No par\u00e2metro CONTAINER_COLOR, pode adicionar as seguintes cores:</p> <ul> <li>red</li> <li>orange</li> <li>yellow</li> <li>olive</li> <li>green</li> <li>teal</li> <li>blue</li> <li>violet</li> <li>purple</li> <li>pink</li> <li>black</li> </ul> </li> <li> <p>No par\u00e2metro PETS, pode adicionar os seguintes animais:</p> <ul> <li>cows</li> <li>chameleons</li> <li>cowmeleons</li> </ul> </li> <li> <p>J\u00e1 no par\u00e2metro TITLE, basta adicionar o texto desejado.</p> </li> </ul> <p>Para visualizar outros poss\u00edveis par\u00e2metros que podem ser adicionados, basta acessar este reposit\u00f3rio</p>"},{"location":"lbc/traefik/","title":"Configura\u00e7\u00e3o b\u00e1sica do Traefik Dashboard","text":"<p>Para realizar a configura\u00e7\u00e3o do Traefik Dashboard \u00e9 necess\u00e1rio manter a instala\u00e7\u00e3o original do K3S, sem remover o Traefik.</p> <p>Em seguida, crie um arquivo de configura\u00e7\u00e3o no seguinte caminho: <code>/var/lib/rancher/k3s/server/manifests/traefik-config.yaml</code>.</p> <p>Com o arquivo criado, adicione o seguinte conte\u00fado: <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: traefik\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    dashboard:\n      enabled: true\n    ports:\n      traefik:\n        expose: true\n    logs:\n      access:\n        enabled: true\n</code></pre></p> <p>Ap\u00f3s salvar o arquivo, acesse em outro computador na mesma rede o endere\u00e7o do seu servidor K3S conforme o exemplo a seguir: </p> <p>http://192.168.0.80:9000/dashboard/</p>"},{"location":"lbc/files/pandaweb/","title":"Panda Web","text":"<p>Copie a configura\u00e7\u00e3o abaixo e salve no arquivo pandaweb.yaml.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: front-app\n  name: front-app\nspec:\n  revisionHistoryLimit: 2\n  replicas: 3\n  selector:\n    matchLabels:\n      app: front-app\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: front-app\n    spec:\n      containers:\n      - image: ronaldo87/panda-frontend:latest\n        name: front-app\n        ports:\n        - containerPort: 80\n          name: server\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: front-app-service\n  labels:\n    app: front-app\nspec:\n  ports:\n  - name: http\n    port: 80\n    targetPort: 80\n    nodePort: 31125\n  selector:\n    app: front-app\n  type: NodePort\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: front-app-ingress\n  annotations:\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: panda-web.io\n    http:\n      paths:\n      - path: \"/\"\n        pathType: Exact\n        backend:\n          service:\n            name: front-app-service\n            port:\n              number: 80\n</code></pre>"},{"location":"lbc/files/rancherdemo/","title":"Rancher Demo","text":"<p>Copie a configura\u00e7\u00e3o abaixo e salve no arquivo rancherdemo.yaml.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rancher-app\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      name: rancher-app\n  template:\n    metadata:\n      labels:\n        name: rancher-app\n    spec:\n      containers:\n        - name: rancher-demo\n          image: ronaldo87/rancher-demo:latest\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 8080\n              name: web\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 200m\n              memory: 100m # fix here\n            requests:\n              cpu: 100m\n              memory: 100m # fix here\n          env:\n          - name: CONTAINER_COLOR\n            value: orange\n          - name: PETS\n            value: chameleons\n          - name: TITLE\n            value: TCC Load Balancer Demo\n          readinessProbe:\n            httpGet:\n              port: web\n              path: /\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: rancher-app\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: rancher-app\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 90\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: rancher-service\nspec:\n  type: NodePort\n  ports:\n    - name: http\n      port: 8080\n      protocol: TCP\n      targetPort: 8080\n      nodePort: 31120\n  selector:\n    name: rancher-app\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rancher-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: rancher-demo-app.io\n    http:\n      paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: rancher-service\n              port:\n                number: 8080\n</code></pre>"}]}